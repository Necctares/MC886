{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs9E_R5yD48u"
      },
      "source": [
        "# **Assignment \\#1**: Machine Learning MC886\n",
        "University of Campinas (UNICAMP), Institute of Computing (IC)\n",
        "\n",
        "Prof. Sandra Avila, 2020s2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVGH2s7fD_03"
      },
      "source": [
        "## Objective \n",
        "\n",
        "Explore **linear regression** alternatives and come up with the best possible model to the problems, avoiding overfitting. In particular, predict the **price of diamonds** from their attributes (e.g., depth, clarity, color) using the Diamonds dataset (https://www.kaggle.com/shivam2503/diamonds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3XDZRGqEwsk"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The Diamonds dataset contains the prices and attributes of almost 50,000 diamonds.\n",
        "\n",
        "Dataset Information: You should respect the following traininig/test split: 45,000 training examples, and 5,000 test examples.\n",
        "\n",
        "There are 9 attributes as follows: \n",
        "\n",
        "- 1: **carat**: weight of the diamond (0.2-5.01)\n",
        "- 2: **cut**: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "- 3: **color**: diamond color, from J (worst) to D (best)\n",
        "- 4: **clarity**: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
        "- 5: **x**: length in mm (0-10.74)\n",
        "- 6: **y**: width in mm (0-58.9)\n",
        "- 7: **z**: depth in mm (0-31.8)\n",
        "- 8: **depth**: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43-79)\n",
        "- 9: **table**: width of top of diamond relative to widest point (43-95)\n",
        "\n",
        "\n",
        "target **price**: price in US dollars\n",
        "\n",
        "The data is available at\n",
        "https://www.dropbox.com/s/tmz8bkocrpfmfb9/diamonds-dataset.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSGS4brHnAi"
      },
      "source": [
        "## Deadline\n",
        "\n",
        "Monday, October 12th 7 pm. \n",
        "\n",
        "Penalty policy for late submission: You are not encouraged to submit your assignment after due date. However, in case you did, your grade will be penalized as follows:\n",
        "- October 13th 7 pm : grade * 0.75\n",
        "- October 14th 7 pm : grade * 0.5\n",
        "- October 15th 7 pm : grade * 0.25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joN9pvZJIfW5"
      },
      "source": [
        "## Submission\n",
        "\n",
        "On Google Classroom, submit your Jupyter Notebook (in Portuguese or English).\n",
        "\n",
        "**This activity is NOT individual, it must be done in pairs (two-person group).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZgUH3bPGacp"
      },
      "source": [
        "## Activities\n",
        "\n",
        "1. (4 points) Perform Linear Regression. You should implement your solution and compare it with ```sklearn.linear_model.SGDRegressor``` (linear model fitted by minimizing a regularized empirical loss with SGD, http://scikit-learn.org). Keep in mind that friends don't let friends use testing data for training :-)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l82hLBDCV37"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y0QxxH1KgE1"
      },
      "source": [
        "# TODO: Load and preprocess your dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "data = pd.read_csv(\"diamonds-train.csv\", na_values = ['no info', '.'])\n",
        "data = data.replace(['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], [1, 2, 3, 4, 5])\n",
        "data = data.replace(['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], [1, 2, 3, 4, 5, 6, 7, 8])\n",
        "data = data.replace(['J', 'I', 'H', 'G', 'F', 'E', 'D'], [1, 2, 3, 4, 5, 6, 7])\n",
        "data_norm = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
        "\n",
        "data_test = pd.read_csv(\"diamonds-test.csv\", na_values = ['no info', '.'])\n",
        "data_test = data_test.replace(['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], [1, 2, 3, 4, 5])\n",
        "data_test = data_test.replace(['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], [1, 2, 3, 4, 5, 6, 7, 8])\n",
        "data_test = data_test.replace(['J', 'I', 'H', 'G', 'F', 'E', 'D'], [1, 2, 3, 4, 5, 6, 7])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9cpdif9JxFR"
      },
      "source": [
        "# TODO: Linear Regression. Implement your solution. You cannot use scikit-learn libraries.\n",
        "def predict(x,theta):\n",
        "    x=np.c_[np.ones(len(x)),x]\n",
        "    return np.dot(x, theta)\n",
        "\n",
        "def gradient_b(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    gradient = [0]*len(theta)\n",
        "    for i in range(len(y)):\n",
        "        gradient[0] += h[i] - y[i]\n",
        "    for k in range(len(theta)-1):\n",
        "        for j in range(len(y)):\n",
        "            gradient[k+1] += (h[j] - y[j])*x[j][k]\n",
        "    return gradient\n",
        "\n",
        "def cost(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    J = 0\n",
        "    for i in range(len(y)):\n",
        "        J += (h[i]-y[i])**2\n",
        "    J /= 2*len(y)\n",
        "    return J\n",
        "\n",
        "def linear_eq(data,alpha,epochs):\n",
        "    error = []\n",
        "    x = data[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "    y = data['price'].values\n",
        "    t = np.zeros((data.shape[1],1))\n",
        "    for e in range(epochs):\n",
        "        gradient = gradient_b(x,y,t)\n",
        "        t[0] = t[0] - (alpha*gradient[0])/len(y)\n",
        "        for j in range(1,len(t)):\n",
        "            t[j] = t[j] - (alpha*gradient[j])/len(y)\n",
        "        error.append(cost(x,y,t))\n",
        "    return t, error\n",
        "\n",
        "t,e=linear_eq(data,1e-4,20)\n",
        "y = data_test['price'].values\n",
        "x = data_test[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "y_pred = predict(x,t)\n",
        "#print(e[1],\" vs. \", e[-1])\n",
        "plt.scatter(y,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHeCvlsHHmq9"
      },
      "source": [
        "# TODO: Linear Regression. Implement your solution with sklearn.linear_model.SGDRegressor.\n",
        "def sklearn_SGD(data): \n",
        "  from sklearn.linear_model import SGDRegressor\n",
        "  from sklearn.pipeline import make_pipeline\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "  x = data[['carat','cut','color','clarity','x','y','z','depth','table']]\n",
        "  y = data['price']\n",
        "  model = make_pipeline(StandardScaler(), SGDRegressor(alpha=0.01,max_iter=5000, tol=1e-4))\n",
        "  model.fit(x,y)\n",
        "  predict = model.predict(x)\n",
        "  return predict\n",
        "\n",
        "t = sklearn_SGD(data)\n",
        "plt.scatter(y,t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBNZQNImKQeo"
      },
      "source": [
        "\n",
        "> What are the conclusions? (1-2 paragraphs)\n",
        "\n",
        " Em comparação com o a implementação do Scikit-learn, nosso codigo foi bem mais custoso, tanto em execução, quanto no quesito erro. Muito por uma possivel má otimização na hora dos calculos (utilizamos for para o calculo de diversos elementos ao invés de fazer produto linear) tanto quanto na forma do modelo escolhido, já que este modelo é um modelo básico montado em um polinomio de primeira ordem.\n",
        " Quanto ao erro, testamos até 100 epocas, por causa do tempo de execução (muito alto se comparado com o SGD do scikit), tendo uma ideia de que o erro estava sendo atenuado, mas não conseguimos ver o quão bem ele poderia desempenhar com maiores iterações.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrPl7jKgJPW6"
      },
      "source": [
        "\n",
        "2. (2 points) Sometimes, we need some more complex function to make good prediction. Devise and test more complex model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGbg41PMHR9"
      },
      "source": [
        "# TODO: Complex model. Implement your solution. You cannot use scikit-learn libraries.\n",
        "\n",
        "def predict(x,theta):\n",
        "    x=np.c_[np.ones(len(x)),x]\n",
        "    return np.dot(x, theta)\n",
        "\n",
        "def gradient_b(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    gradient = [0]*len(theta)\n",
        "    for i in range(len(y)):\n",
        "        gradient[0] += h[i] - y[i]\n",
        "    for k in range(len(theta)-1):\n",
        "        for j in range(len(y)):\n",
        "            gradient[k+1] += (h[j] - y[j])*x[j][k]\n",
        "    return gradient\n",
        "\n",
        "def cost(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    J = 0\n",
        "    for i in range(len(y)):\n",
        "        J += (h[i]-y[i])**2\n",
        "    J /= 2*len(y)\n",
        "    return J\n",
        "\n",
        "def matrix_union(A, B):\n",
        "    for a, b in zip(A, B):\n",
        "        yield [*a, *b]\n",
        "\n",
        "def linear_eq_grau2(data,alpha,epochs):\n",
        "    error = []\n",
        "    x = data[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "    y = data['price'].values\n",
        "    t = np.zeros((data.shape[1]*2-1,1))\n",
        "    c = x*x\n",
        "    x = list(matrix_union(x,c))\n",
        "    for e in range(epochs):\n",
        "        error.append(cost(x,y,t))\n",
        "        gradient = gradient_b(x,y,t)\n",
        "        t[0] = t[0] - (alpha*gradient[0])/len(y)\n",
        "        for j in range(1,len(t)):\n",
        "            t[j] = t[j] - (alpha*gradient[j])/len(y)\n",
        "    return t, error, x\n",
        "\n",
        "t,e,x=linear_eq_grau2(data,0.0001,10)\n",
        "y = data['price'].values\n",
        "y_pred = predict(x,t)\n",
        "#print(e[1],\" vs. \", e[-1])\n",
        "plt.scatter(y,y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUX0AraaNH-e"
      },
      "source": [
        "> What are the conclusions? (1-2 paragraphs)\n",
        "\n",
        "Observamos que houve um contraste em relação ao comportamento do erro quanto a função implementada de grau 1, neste modelo, o erro variou em certas faixas para um valor maior e em outras para menos. Mudamos os valores das iterações entre o intervalo [1-20] e vimos que entre os valores 5-8 foram apresentadas as melhores taxas de erro, mas ainda sim, para esse conjunto de dados, a nossa implementação mais simples, com todas as features em grau 1, obtivemos um resultado melhor, mas longe do esperado. Esta má otimização pode ter ocorrido devido a nossa escolha de colocar todas as features em um grau maior, ao inves, de analisar primeiro os dados e ver qual modelo polinomial melhor se encaixava em cada uma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldSh1vtWK5Zk"
      },
      "source": [
        "3. (1 point) Plot the cost function vs. number of iterations in the training set and analyze the model complexity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg7aNkl_LG4P"
      },
      "source": [
        "# TODO: Plot the cost function vs. number of iterations in the training set.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "\n",
        "t,e,x1=linear_eq_grau2(data,0.0001,10)\n",
        "\n",
        "y = e\n",
        "x = np.zeros(len(y))\n",
        "for i in range(len(y)):\n",
        "  x[i] = i+1\n",
        "\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('error')\n",
        "\n",
        "plt.plot(x, y, 'o', color='blue');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBLKtosaLaCw"
      },
      "source": [
        "\n",
        " > What are the conclusions? What are the actions after such analyses? (1-2 paragraphs)\n",
        "\n",
        " Observamos que em media, entre as iterações a taxa de erro variava pouco em comparação com iterações maiores, apartir de iterações maiores (10 ou mais), o erro explodia para casas muito altas, ficando claro que há um erro no modelo. A melhor faixa de valores para o erro, foi entre as iterações 5 e 8, mas o modelo linear mais simples teve melhor aproveitamento.\n",
        "\n",
        " Tais falhas podem ter sido geradas no modo de implementação, ou mesmo no modelo proposto, em que todas as features assumiam valores polinomiais de grau 2 em vez de analisarmos cada feature e tentar prever um polinomio especifico para a mesma.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADxPBRhuK_Vq"
      },
      "source": [
        "4. (3 points) Use different Gradient Descent (GD) learning rates when optimizing. Compare the GD-based solutions with Normal Equation. You should implement your solutions. What are the conclusions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSZ1pLItNVbU"
      },
      "source": [
        "# TODO: Gradient Descent (GD) with different learning rates. Implement your solution. You cannot use scikit-learn libraries.\n",
        "\n",
        "#Mini Batch\n",
        "def create_mini_batch(data,batch_size):\n",
        "    m_b = []\n",
        "    x = data[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "    y = data['price'].values\n",
        "    batch = np.c_[x,y]\n",
        "    np.random.shuffle(batch)\n",
        "    number_of_batches = batch.shape[0]//batch_size\n",
        "    i=0\n",
        "    for i in range(number_of_batches+1):\n",
        "        mini_batch = batch[i * batch_size:(i + 1)*batch_size, :] \n",
        "        x_m = mini_batch[:, :-1]\n",
        "        y_m = mini_batch[:, -1].reshape((-1, 1))\n",
        "        m_b.append((x_m, y_m))\n",
        "    if data.shape[0] % batch_size != 0: \n",
        "        mini_batch = batch[i * batch_size:data.shape[0]] \n",
        "        x_mini = mini_batch[:, :-1] \n",
        "        y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        m_b.append((x_mini, y_mini)) \n",
        "    return m_b\n",
        "\n",
        "def predict(x,theta):\n",
        "    x=np.c_[np.ones(len(x)),x]\n",
        "    return np.dot(x, theta)\n",
        "\n",
        "def gradient_mb(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    gradient = [0]*len(theta)\n",
        "    for i in range(len(y)):\n",
        "        gradient[0] += h[i] - y[i]\n",
        "    for k in range(len(theta)-1):\n",
        "        for j in range(len(y)):\n",
        "            gradient[k+1] += (h[j] - y[j])*x[j][k]\n",
        "    return gradient\n",
        "\n",
        "def cost(x, y, theta): \n",
        "    h = predict(x, theta)\n",
        "    J = 0\n",
        "    for i in range(len(y)):\n",
        "        J += (h[i]-y[i])**2\n",
        "    J /= 2*len(y)\n",
        "    return J\n",
        "\n",
        "def mini_batch_gd(data,alpha,epochs,batch_size):\n",
        "    error = []\n",
        "    t = np.zeros((data.shape[1],1))\n",
        "    for i in range(epochs):\n",
        "        mini_batches = create_mini_batch(data,batch_size)\n",
        "        for batch in mini_batches:\n",
        "            x,y = batch\n",
        "            gradient = gradient_mb(x,y,t)\n",
        "            if len(y) > 0:\n",
        "              t[0] = t[0] - (alpha*gradient[0])/len(y)\n",
        "              for j in range(1,len(t)):\n",
        "                  t[j] = t[j] - (alpha*gradient[j])/len(y)\n",
        "              error.append(cost(x,y,t))\n",
        "    return t, error\n",
        "\n",
        "#SGD\n",
        "\n",
        "def predict_sgd(x,theta):\n",
        "    x=np.r_[1,x]\n",
        "    return np.dot(x,theta)\n",
        "\n",
        "def gradient_sgd(x, y, theta): \n",
        "    h = predict_sgd(x, theta)\n",
        "    gradient = [0]*len(theta)\n",
        "    gradient[0] = (h - y)\n",
        "    for k in range(len(theta)-1):\n",
        "        gradient[k+1] += (h - y)*x[k]\n",
        "    return gradient\n",
        "\n",
        "def cost_sgd(x, y, theta): \n",
        "    h = predict_sgd(x, theta)\n",
        "    J = (h-y)**2\n",
        "    J /= 2\n",
        "    return J\n",
        "\n",
        "def stochastic_gd(data,alpha,epochs):\n",
        "    data_shuff = data.values\n",
        "    error = []\n",
        "    t = np.zeros((data.shape[1],1))\n",
        "    for i in range(epochs):\n",
        "        np.random.shuffle(data_shuff)\n",
        "        for example in data_shuff:\n",
        "            x = example[:-1]\n",
        "            y = example[-1]\n",
        "            gradient = gradient_sgd(x,y,t)\n",
        "            t[0] = t[0] - (alpha*gradient[0])\n",
        "            for j in range(1,len(t)):\n",
        "              t[j] = t[j] - (alpha*gradient[j])\n",
        "            error.append(cost_sgd(x,y,t))\n",
        "    return t, error\n",
        "\n",
        "x = data_test[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "y = data_test['price'].values\n",
        "t, error = stochastic_gd(data,0.0001,20)\n",
        "#t, error = mini_batch_gd(data,0.0001,20,500):\n",
        "y_pred = predict(x,t)\n",
        "plt.scatter(y,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-xRuMOVNlJP"
      },
      "source": [
        "# TODO: Compare the GD-based solutions (e.g., Batch, SGD, Mini-batch) with Normal Equation. Implement your solution. You cannot use scikit-learn libraries.\n",
        "\n",
        "def normal_eq(data):\n",
        "  #Normal equation implementation\n",
        "  x = data[['carat','cut','color','clarity','x','y','z','depth','table']].values\n",
        "  y = data['price'].values\n",
        "  x=np.c_[np.ones(len(y)),x]\n",
        "  #Calculo do (x_transposto*x)^(-1)\n",
        "  x_trans = np.transpose(x)\n",
        "  x_trans_esc = x_trans.dot(x)\n",
        "  #Calculo do (x_transposto*x)^(-1) * (x_transposto*y)\n",
        "  c1 = np.linalg.inv(x_trans_esc)\n",
        "  c2 = x_trans.dot(y)\n",
        "  result = c1.dot(c2)\n",
        "  return result\n",
        "\n",
        "#Teste\n",
        "result = normal_eq(data)\n",
        "y_pred = predict(x,result)\n",
        "plt.scatter(y,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbGOQzcsNSOm"
      },
      "source": [
        "# > What are the conclusions? (2-4 paragraphs)\n",
        "\n",
        "  Observando o comportamento das funções com GD Mini batch e Estocastico, temos que as duas só convergiram com uma learning rate menor que 1e-3, como o estocastico atualiza seu theta por cada iteração, obtivemos um resultado melhor para o erro por epoca em relação ao mini batch, porém exigiu um maior tempo para executa-lo.\n",
        "\n",
        "  Comparando a execução dos dois GD com o metodo da Equação normal, temos que a equação normal obteve um resultado muito melhor que os outros dois, inclusive, tendo desempenho parecido com a função SGD implementada do Scikit Learn. Foi muito mais rapido, além de não ter problemas de divergencia com learning rate e ter um custo de processamento bem menor para o conjunto de dados usado, pois não precisa iterar.\n",
        "\n",
        "  Então, para um conjunto de dados relativamente pequeno, a equação normal tende a ser uma melhor alternativa, porém, temos que levar em conta que é um processo de computação matricial, então, se tivermos uma matriz de ordem relativamente grande, teremos um processo mais lento, além de algumas matrizes não serem invertiveis, impossibilitando assim, seu calculo.\n"
      ]
    }
  ]
}